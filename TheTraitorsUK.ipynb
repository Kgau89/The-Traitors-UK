{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **The Traitors UK Analysis**"
      ],
      "metadata": {
        "id": "suTuwFS6mbbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkEXT-m6Eet6"
      },
      "outputs": [],
      "source": [
        "!pip3 -q install snscrape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neattext"
      ],
      "metadata": {
        "id": "g0_QIvkSGNgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "import nltk as nlp\n",
        "nlp.download('all')\n",
        "nlp.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import requests\n",
        "import string\n",
        "import neattext as nt\n",
        "import neattext.functions as nfx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPwAIpUvE8nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Gathering**"
      ],
      "metadata": {
        "id": "FAyQFqTQmqUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create scraper object with keywords \n",
        "scraper = sntwitter.TwitterSearchScraper(\"#TheTraitorsUK lang:en until:2022-12-30 since:2022-11-03\")\n",
        "\n",
        "tweets = []\n",
        "\n",
        "for i, tweet in enumerate(scraper.get_items()):\n",
        "  data = [\n",
        "      tweet.id,\n",
        "      tweet.date,\n",
        "      tweet.content,\n",
        "      tweet.user.location,\n",
        "      tweet.likeCount,\n",
        "      tweet.retweetCount\n",
        "  ]\n",
        "  tweets.append(data)\n",
        "  if i>10:\n",
        "    break\n",
        "\n",
        "#Create DataFrame\n",
        "tweet_df = pd.DataFrame(tweets,\n",
        "                        columns = ['tweet_id','date','TweetText','location','likes','retweets'])\n",
        "\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "g6A9XdGFHzdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check data shape\n",
        "tweet_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF-uGaHNLrJw",
        "outputId": "cc5c448f-4cd0-4d72-fca9-f4e38eb36f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5152, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "f3PMel0OA6aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to extract hashtags \n",
        "def getHashtags(tweet):\n",
        "    tweet = tweet.lower()  \n",
        "    tweet = re.findall(r'\\#\\w+',tweet) \n",
        "    return \" \".join(tweet)\n",
        "\n",
        "tweet_df['Hashtags'] = tweet_df['TweetText'].apply(getHashtags)\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "H6eYCgtzxRo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the count of hashtags\n",
        "hashtagPattern = re.compile(r'#(\\w+)')\n",
        "\n",
        "\n",
        "hashtags_list = tweet_df['Hashtags'].tolist()\n",
        "\n",
        "hashtags = []\n",
        "for item in hashtags_list:\n",
        "    item = item.split()\n",
        "    for i in item:\n",
        "        hashtags.append(i)\n",
        "\n",
        "words = [word for ht in hashtags for word in hashtagPattern.findall(ht)]\n",
        "\n",
        "counted = Counter(words)\n",
        "hashtags_df = pd.DataFrame.from_dict(counted, orient='index').reset_index()\n",
        "hashtags_df.columns = ['Hashtags', 'Count']\n",
        "hashtags_df.sort_values(by='Count', ascending=False, inplace=True)\n",
        "hashtags_df"
      ],
      "metadata": {
        "id": "mDQ94OdXQ9dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Contestants\n",
        "cast =['amos', 'maddy', 'fay', 'ivan', 'john', 'theo', 'kieran', 'andrea','wilf','wilfred', \n",
        "          'meryl', 'alyssa', 'tom', 'aisha', 'imran', 'alex', 'claire', 'nicky', 'matt', 'amanda', 'rayan','hannah', 'aaron']\n",
        "\n",
        "\n",
        "\n",
        "def getcast(tweet):\n",
        "    tweet = tweet.lower() \n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    castM = [char for char in tweet_tokens if char in cast] \n",
        "    return \" \".join(castM)\n",
        "\n",
        "# Extract casts to a new column\n",
        "tweet_df['Traitors_Cast'] = tweet_df['TweetText'].apply(getcast)\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "zgKE7wZFyCIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Cast member references\n",
        "cast_list = tweet_df['Traitors_Cast'].tolist()\n",
        "\n",
        "\n",
        "cast = []\n",
        "for item in cast_list:\n",
        "    item = item.split()\n",
        "    for i in item:\n",
        "        cast.append(i)\n",
        "\n",
        "\n",
        "counts = Counter(cast)\n",
        "cast_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
        "cast_df.columns = ['Traitors_Cast', 'Count']\n",
        "cast_df.sort_values(by='Count', ascending=False, inplace=True)\n",
        "cast_df.head(10)"
      ],
      "metadata": {
        "id": "bjXR61IP01QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**\n"
      ],
      "metadata": {
        "id": "BfdANZlPm_Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Cleaning\n",
        "def cleanTweets(twt):\n",
        "  twt = twt.lower()\n",
        "  twt = re.sub('RT', '',twt) #Remove RT\n",
        "  twt = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', twt, flags = re.MULTILINE) #Remove hyperlinks\n",
        "  twt = re.sub('\\\\n','',twt) #Remove '\\n' character\n",
        "  twt = re.sub(r'\\#\\w+','',twt) #Remove hashtags\n",
        "  twt = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '', twt)\n",
        "  twt = re.sub('[^a-zA-Z]', '', twt) #remove all other letters except alphabets and numbers\n",
        "  twt = re.sub('@[\\S]*','',twt) #Remove @mentions\n",
        "  twt = re.sub('^[\\s]+|[\\s]+$','',twt) #Remove leading and trailing white spaces\n",
        "  twt = re.sub(\"\\'\", '', twt) #remove single quotes\n",
        "  twt = re.sub('\"', '', twt) #remove double quotes\n",
        "  twt = re.sub('[()!?]', ' ', twt) #remove punctuations\n",
        "  twt = re.sub('\\[.*?\\]',' ', twt) #remove punctuations\n",
        "  return twt"
      ],
      "metadata": {
        "id": "a54j3bwWpFJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Emojis\n",
        "def strip_emoji(twt):\n",
        "    RE_EMOJI = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
        "    return RE_EMOJI.sub(r'', twt)"
      ],
      "metadata": {
        "id": "jCQQA5Z3ys7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Column for Cleaned Tweets\n",
        "tweet_df['Cleaned_Tweets'] = tweet_df['TweetText'].apply(cleanTweets)\n",
        "tweet_df['Cleaned_Tweets'] = tweet_df['TweetText'].apply(strip_emoji)\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "lyVu-U6312SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Noise scan\n",
        "tweet_df['Cleaned_Tweets'].apply(lambda x: nt.TextFrame(x).noise_scan()['text_noise'])"
      ],
      "metadata": {
        "id": "2-CswVedzWU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Stopwords\n",
        "tweet_df['Cleaned_Tweets'].apply(lambda x: nt.TextExtractor(x).extract_stopwords(lang='en'))"
      ],
      "metadata": {
        "id": "uPlKKiUQ8xSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List Stopwords\n",
        "stop_words = ['co','s','t','n',\"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
        "                   \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
        "                   \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \n",
        "                   \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
        "                   \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \n",
        "                   \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
        "                   \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \n",
        "                   \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\"can\", \"will\", \"just\",\"should\",\n",
        "                   \"now\",'anyone','today','yesterday','day', 'already',\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \n",
        "                   \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "                   'traitor','thetraitor','faithful','u', 'uk','bbc','traitors','thetraitors',\n",
        "                   'thetraitorsuk']\n",
        "letters = list(string.ascii_lowercase)\n",
        "all_stopwords = stop_words + letters"
      ],
      "metadata": {
        "id": "3dvc7_Y4HaiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "rbVRuaYMAkKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data PreProcessing\n",
        "def processedTweets(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  #Remove Stopwords and punctuation\n",
        "  tweet_tokens = word_tokenize(tweet)\n",
        "  filtered_tokens = [t for t in tweet_tokens if t not in all_stopwords]\n",
        "  unpunct_tokens = [t for t in filtered_tokens if t not in string.punctuation]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemma_tokens = [lemmatizer.lemmatize(t) for t in unpunct_tokens]\n",
        "  return ' '.join(lemma_tokens)\n"
      ],
      "metadata": {
        "id": "rukfUk6rXZpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply processed tweets to dataframe\n",
        "tweet_df['Processed_Tweets'] = tweet_df['Cleaned_Tweets'].apply(processedTweets)\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "mqk202gDewky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get full content\n",
        "Fulltweet = tweet_df['Processed_Tweets'].tolist()\n",
        "Fulltweet = ' '.join(Fulltweet)\n",
        "tweet_df"
      ],
      "metadata": {
        "id": "h1Zhu4aXl00O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Noise scan\n",
        "tweet_df['Processed_Tweets'].apply(lambda x: nt.TextFrame(x).noise_scan()['text_noise'])"
      ],
      "metadata": {
        "id": "2LOo4XziOS6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop non-required columns\n",
        "tweet_df.drop(['TweetText','Cleaned_Tweets'], axis=1, inplace=False)\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "Kh60Q0Mbkd_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "9eT7jVL_AsVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Polarity Score\n",
        "def getPolarity(tweet):\n",
        "    return TextBlob(tweet).sentiment.polarity\n",
        "\n",
        "#Sentiment category\n",
        "def getSentimentTextBlob(polarity):\n",
        "    if polarity < 0:\n",
        "        return \"Negative\"\n",
        "    elif polarity == 0:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Positive\""
      ],
      "metadata": {
        "id": "QEu2x2UVP3Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create columns for Polarity Score and Sentiment\n",
        "tweet_df['Polarity']=tweet_df['Processed_Tweets'].apply(getPolarity)\n",
        "tweet_df['Sentiment']=tweet_df['Polarity'].apply(getSentimentTextBlob)\n",
        "tweet_df['Sentiment'].value_counts()\n",
        "tweet_df.head()"
      ],
      "metadata": {
        "id": "53JoIUMmQBIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save file to Excel\n",
        "tweet_df.to_csv('TheTraitors.csv')"
      ],
      "metadata": {
        "id": "rzVkh7dnZUUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create WordCloud\n",
        "wc= WordCloud(collocations = False,max_words=500, background_color = 'black')\n",
        "wc.generate(Fulltweet)\n",
        " \n",
        "# plot the WordCloud image                      \n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MuIuFHZXQwA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc.to_file('wordcloud.png')"
      ],
      "metadata": {
        "id": "7yVVTiBjZ-K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe3fbef-2e10-4642-8da0-75ba59883811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wordcloud.wordcloud.WordCloud at 0x7f8f06c7bc10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "The exported data frame will be used in Tableau to create a dashboard and visualise results.\n"
      ],
      "metadata": {
        "id": "EbFVvzpCAA0s"
      }
    }
  ]
}